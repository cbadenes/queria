{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.2.0-py3-none-any.whl (9.5 kB)\n",
      "Collecting httpx<0.28.0,>=0.27.0\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /Users/cbadenes/miniforge3/envs/.muheqa/lib/python3.9/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.3)\n",
      "Requirement already satisfied: certifi in /Users/cbadenes/miniforge3/envs/.muheqa/lib/python3.9/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2022.6.15)\n",
      "Requirement already satisfied: sniffio in /Users/cbadenes/miniforge3/envs/.muheqa/lib/python3.9/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.0)\n",
      "Requirement already satisfied: anyio in /Users/cbadenes/miniforge3/envs/.muheqa/lib/python3.9/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.6.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/cbadenes/miniforge3/envs/.muheqa/lib/python3.9/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Installing collected packages: httpcore, httpx, ollama\n",
      "Successfully installed httpcore-1.0.5 httpx-0.27.0 ollama-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Configuración básica del logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def log(message):\n",
    "    print(f\"{datetime.now()} - {message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "roles = [\"sys\",\"human\"]\n",
    "levels = [\"hard\", \"medium\", \"easy\"]\n",
    "question_format_template   = Path(\"question_format_template.txt\").read_text().strip()\n",
    "\n",
    "prompt = {}\n",
    "for role in roles:\n",
    "    prompt[role] = {}\n",
    "    for level in levels:\n",
    "        prompt_text = Path(role + \"_template_\"+level+\".txt\").read_text().strip()\n",
    "        if (role == \"human\"):\n",
    "            prompt_text += question_format_template\n",
    "        prompt[role][level] = prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def chat(server,role, message, model:\"llama3\"):\n",
    "    messages = [\n",
    "    {\n",
    "        'role': role,\n",
    "        'content': message,\n",
    "    },\n",
    "    ]\n",
    "    log(\"making a request to chat: \" + model)\n",
    "    r = requests.post(\n",
    "        server + \"/api/chat\",\n",
    "        json={\"model\": model,  \"messages\": messages, \"stream\": True},\n",
    "    )\n",
    "    log(\"response received from chat\")\n",
    "    r.raise_for_status()\n",
    "    output = \"\"\n",
    "\n",
    "    for line in r.iter_lines():\n",
    "        body = json.loads(line)\n",
    "        if \"error\" in body:\n",
    "            raise Exception(body[\"error\"])\n",
    "        if body.get(\"done\") is False:\n",
    "            response = body.get(\"message\", \"\")\n",
    "            content = response.get(\"content\", \"\")\n",
    "            output += content\n",
    "            # the response streams one token at a time, print that as we receive it\n",
    "            #print(content, end=\"\", flush=True)\n",
    "\n",
    "        if body.get(\"done\", False):\n",
    "            response[\"content\"] = output\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def parse_question(chat_response):\n",
    "    response = chat_response.replace(\"\\n\",\"\")\n",
    "    json_string = response.split(\"{\")[1].split(\"}\")[0]\n",
    "    question = json.loads(\"{\" + json_string + \"}\")\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question(server,context, level, model):\n",
    "    chat(server,\"system\",prompt[\"sys\"][level], model)\n",
    "    response = chat(server,\"user\",prompt[\"human\"][level].replace('{context}',context), model)\n",
    "    return parse_question(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 17:34:57 - DEBUG - Starting new HTTPS connection (1): librairy.linkeddata.es:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting  hard role in model:llama3\n",
      "2024-06-05 17:34:57.831141 - making a request to chat: llama3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 17:35:47 - DEBUG - https://librairy.linkeddata.es:443 \"POST /ollama/api/chat HTTP/1.1\" 200 None\n",
      "2024-06-05 17:37:04 - DEBUG - Starting new HTTPS connection (1): librairy.linkeddata.es:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-05 17:37:04.298772 - response received from chat\n",
      "role  hard completed  in model:llama3\n",
      "making a hard question ..\n",
      "2024-06-05 17:37:04.302493 - making a request to chat: llama3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 17:37:46 - DEBUG - https://librairy.linkeddata.es:443 \"POST /ollama/api/chat HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-05 17:38:45.256302 - response received from chat\n",
      "{\n",
      "  \"PREGUNTA\": \"¿Qué organización sin ánimo de lucro administra los servidores de la Wikipedia en español?\",\n",
      "  \"OPCION1\": \"La Universidad Nacional Autónoma de México\",\n",
      "  \"OPCION2\": \"El Instituto Nacional de Estadística\",\n",
      "  \"OPCION3\": \"La Fundación para el Desarrollo del Conocimiento\",\n",
      "  \"OPCION4\": \"La Fundación Wikimedia\",\n",
      "  \"EVIDENCIA\": \"La Fundación Wikimedia es la organización sin ánimo de lucro que administra los servidores de la Wikipedia en español, según se indica en el texto.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "sample = context = \"La Wikipedia en español es la edición en español o castellano de Wikipedia. Al igual que las versiones existentes de Wikipedia en otros idiomas, es una enciclopedia de contenido libre, publicada en Internet bajo las licencias libres CC BY-SA 4.0 y GFDL. En la actualidad cuenta con 1 949 592 artículos, y es escrita por usuarios voluntarios, es decir, que cualquiera puede editar un artículo, corregirlo o ampliarlo. Los servidores son administrados por la Fundación Wikimedia, una organización sin ánimo de lucro cuya financiación se basa fundamentalmente en donaciones.\"\n",
    "levels = [\"hard\", \"medium\", \"easy\"] \n",
    "\n",
    "\n",
    "#server = \"http://0.0.0.0:11434\" #local\n",
    "server = \"https://librairy.linkeddata.es/ollama\" #remote\n",
    "\n",
    "model = \"llama3\"\n",
    "\n",
    "print(\"setting  \" + level + \" role in model:\" + model)\n",
    "chat(server,\"system\",prompt[\"sys\"][level], model)\n",
    "print(\"role  \" + level + \" completed  in model:\" + model)\n",
    "\n",
    "print(\"making a \" + level + \" question ..\")\n",
    "question = create_question(server,sample, level, model)\n",
    "print(json.dumps(question,indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 17:27:32 - DEBUG - Starting new HTTPS connection (1): librairy.linkeddata.es:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making a hard question ..\n",
      "2024-06-05 17:27:32.101925 - making a request to chat: llama3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 17:28:32 - DEBUG - https://librairy.linkeddata.es:443 \"POST /ollama/api/chat HTTP/1.1\" 504 176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-05 17:28:32.167610 - response received from chat\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "504 Server Error: Gateway Time-out for url: https://librairy.linkeddata.es/ollama/api/chat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level \u001b[38;5;129;01min\u001b[39;00m levels:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaking a \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m level \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m question ..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver\u001b[49m\u001b[43m,\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(question,indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m, in \u001b[0;36mcreate_question\u001b[0;34m(server, context, level, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_question\u001b[39m(server,context, level, model:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     response \u001b[38;5;241m=\u001b[39m chat(server,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m][level]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{context}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,context), model)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_question(response)\n",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m, in \u001b[0;36mchat\u001b[0;34m(server, role, message, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     13\u001b[0m     server \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse received from chat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n",
      "File \u001b[0;32m~/miniforge3/envs/.muheqa/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 504 Server Error: Gateway Time-out for url: https://librairy.linkeddata.es/ollama/api/chat"
     ]
    }
   ],
   "source": [
    "import json\n",
    "sample = context = \"La Wikipedia en español es la edición en español o castellano de Wikipedia. Al igual que las versiones existentes de Wikipedia en otros idiomas, es una enciclopedia de contenido libre, publicada en Internet bajo las licencias libres CC BY-SA 4.0 y GFDL. En la actualidad cuenta con 1 949 592 artículos, y es escrita por usuarios voluntarios, es decir, que cualquiera puede editar un artículo, corregirlo o ampliarlo. Los servidores son administrados por la Fundación Wikimedia, una organización sin ánimo de lucro cuya financiación se basa fundamentalmente en donaciones.\"\n",
    "levels = [\"hard\", \"medium\", \"easy\"] \n",
    "\n",
    "\n",
    "#server = \"http://0.0.0.0:11434\" #local\n",
    "server = \"https://librairy.linkeddata.es/ollama\" #remote\n",
    "\n",
    "model = \"llama3\"\n",
    "\n",
    "for level in levels:\n",
    "    print(\"making a \" + level + \" question ..\")\n",
    "    question = create_question(server,sample, level, model)\n",
    "    print(json.dumps(question,indent=2, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
